{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f44bb05",
   "metadata": {},
   "source": [
    "# Machine Learning 8 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bed96b6",
   "metadata": {},
   "source": [
    "#### What exactly is a feature? Give an example to illustrate your point.\n",
    "\n",
    "#### Ans:\n",
    "\n",
    "\n",
    "Features are the basic building blocks of datasets. The quality of the features in your dataset has a major impact on the quality of the insights you will gain when you use that dataset for machine learning.\n",
    "\n",
    "Additionally, different business problems within the same industry do not necessarily require the same features, which is why it is important to have a strong understanding of the business goals of your data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4e2e9",
   "metadata": {},
   "source": [
    "In general, a feature is a distinctive attribute or characteristic of something that sets it apart from others. In the context of computer science, a feature often refers to a specific aspect or functionality of a software program or application.\n",
    "\n",
    "For **example**, if we consider a music streaming application, some of the features could include:\n",
    "\n",
    "1.Search functionality to find songs, albums, or artists\n",
    "\n",
    "2.Personalized playlists based on the user's listening history\n",
    "\n",
    "3.Recommendations for new music based on the user's preferences\n",
    "\n",
    "4.Social features to share and discover music with friends\n",
    "\n",
    "5.Offline playback so users can listen without an internet connection\n",
    "\n",
    "6.Each of these features is a distinct aspect of the music streaming application that provides a specific benefit or functionality to the user.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3982a3b6",
   "metadata": {},
   "source": [
    "#### 2. What are the various circumstances in which feature construction is required ?\n",
    "\n",
    "**Ans:** Feature construction or feature engineering is the process of selecting, extracting, and transforming raw data into useful features that can be used in a machine learning model. Feature construction can be required in various circumstances, including:\n",
    "\n",
    "**Insufficient or irrelevant features:** When the available features are not sufficient to explain the variability in the data or are irrelevant to the task at hand, new features need to be constructed.\n",
    "\n",
    "**Domain-specific knowledge:** In some cases, domain-specific knowledge is required to engineer meaningful features. For example, in image recognition tasks, features such as texture, shape, and color may need to be constructed to represent the image content.\n",
    "\n",
    "**Data quality:** If the quality of the available data is poor, feature construction may be necessary to remove noise, fill in missing values, or handle outliers.\n",
    "\n",
    "**Dimensionality reduction:** High-dimensional data can be difficult to work with and may lead to overfitting. Feature construction can be used to reduce the dimensionality of the data while preserving the relevant information.\n",
    "\n",
    "**Feature representation:** In some cases, the raw data may not be suitable for use in a machine learning model. Feature construction can be used to transform the data into a suitable representation, such as a bag-of-words model for text data.\n",
    "\n",
    "\n",
    "\n",
    "The features in your data will directly influence the predictive models you use and the results you can achieve. Your results are dependent on many inter-dependent properties. You need great features that describe the structures inherent in your data. Better features means flexibility.\n",
    "\n",
    "\n",
    "\n",
    "### 3. Describe how nominal variables are encoded ?\n",
    "\n",
    "\n",
    "**Ans:** Nominal data is made of discrete values with no numerical relationship between the different categories — mean and \n",
    "\n",
    "median are meaningless. Animal species is one example. For example, pig is not higher than bird and lower than fish.\n",
    "\n",
    "\n",
    "#### 4. Describe how numeric features are converted to categorical features ?\n",
    "\n",
    "\n",
    "**Ans:** Converting categorical features into numeric features using domain knowledge. For example, we are given a list of \n",
    "\n",
    "countries and say we know the distance to these countries from India then we can replace it with distance from India. So, every \n",
    "\n",
    "country can be represented as its distance from India.\n",
    "\n",
    "#### 5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach ?\n",
    "\n",
    "**Ans:** Wrapper methods measure the “usefulness” of features based on the classifier performance. In contrast, the filter \n",
    "\n",
    "methods pick up the intrinsic properties of the features (i.e., the “relevance” of the features) measured via univariate \n",
    "\n",
    "statistics instead of cross-validation performance.\n",
    "\n",
    "\n",
    "The wrapper classification algorithms with joint dimensionality reduction and classification can also be used but these methods \n",
    "\n",
    "have high computation cost, lower discriminative power. Moreover, these methods depend on the efficient selection of \n",
    "\n",
    "classifiers for obtaining high accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07056bbf",
   "metadata": {},
   "source": [
    "#### Advantages of the feature selection wrapper approach include:\n",
    "\n",
    "1. Improved model performance: By selecting a smaller subset of relevant features, the model can be more focused and accurate.\n",
    "\n",
    "2. Reduced overfitting: By selecting only the most important features, the model is less likely to overfit the training data.\n",
    "\n",
    "3. Reduced computational cost: By reducing the number of features, the computational cost of training and running the model can be reduced.\n",
    "\n",
    "4. Interpretable models: With a smaller set of features, the resulting model is more interpretable and easier to understand.\n",
    "\n",
    "\n",
    "#### Disadvantages of the feature selection wrapper approach include:\n",
    "\n",
    "1. Computationally expensive: The wrapper approach can be computationally expensive, as it requires training and evaluating multiple models.\n",
    "\n",
    "2. Sensitive to the choice of algorithm: The wrapper approach can be sensitive to the choice of algorithm used for selecting the feature subsets.\n",
    "\n",
    "3. May miss important features: The wrapper approach may miss important features that are not included in any of the selected subsets.\n",
    "\n",
    "4. May not generalize well: The wrapper approach may select a subset of features that work well on the training data but do not generalize well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d72f299",
   "metadata": {},
   "source": [
    "#### 6. When is a feature considered irrelevant? What can be said to quantify it ?\n",
    "\n",
    "**Ans:** Features are considered relevant if they are either strongly or weakly relevant, and are considered irrelevant otherwise.\n",
    "\n",
    "Irrelevant features can never contribute to prediction accuracy, by definition. Also to quantify it we need to first check the list of features, There are three types of feature selection:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5068aeaa",
   "metadata": {},
   "source": [
    "**Wrapper methods** (forward, backward, and stepwise selection)\n",
    "\n",
    "**Filter methods** (ANOVA, Pearson correlation, variance thresholding)\n",
    "\n",
    "**Embedded methods** (Lasso, Ridge, Decision Tree)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6713d518",
   "metadata": {},
   "source": [
    "#### 7. When is a function considered redundant? What criteria are used to identify features that could be redundant ?\n",
    "\n",
    "**Ans:**  If two features {X1, X2} are highly correlated, then the two features become redundant features since they have same \n",
    "\n",
    "information in terms of correlation measure. In other words, the correlation measure provides statistical association between \n",
    "\n",
    "any given a pair of features.\n",
    "\n",
    "Minimum redundancy feature selection is an algorithm frequently used in a method to accurately identify characteristics of \n",
    "\n",
    "genes and phenotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d97d34c",
   "metadata": {},
   "source": [
    "#### 8. What are the various distance measurements used to determine feature similarity ?\n",
    "\n",
    "**Ans:** Four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "1.Hamming Distance.\n",
    "\n",
    "2.Euclidean Distance\n",
    "\n",
    "3.Manhattan Distance.\n",
    "\n",
    "4.cosine Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a442fb",
   "metadata": {},
   "source": [
    "#### 1. Hamming Distance\n",
    "Hamming distance is a metric for comparing two binary data strings. While comparing two binary strings of equal length, Hamming \n",
    "\n",
    "distance is the number of bit positions in which the two bits are different.\n",
    "\n",
    "The Hamming distance between two strings, a and b is denoted as d(a,b)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3f042c",
   "metadata": {},
   "source": [
    "#### 2. Euclidean Distance\n",
    "\n",
    "In Mathematics, the Euclidean distance is defined as the distance between two points. In other words, the Euclidean distance \n",
    "\n",
    "between two points in the Euclidean space is defined as the length of the line segment between two points. As the Euclidean \n",
    "\n",
    "distance can be found by using the coordinate points and the Pythagoras theorem, it is occasionally called the Pythagorean \n",
    "\n",
    "distance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf44fe5",
   "metadata": {},
   "source": [
    "#### 3.Manhattan Distance\n",
    "\n",
    "Manhattan distance is a distance measure that is calculated by taking the sum of distances between the x and y coordinates.\n",
    "\n",
    "The Manhattan distance is also known as Manhattan length. In other words, it is the distance between two points measured along axes at right angles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77eca0a",
   "metadata": {},
   "source": [
    "4. **Cosine similarity** is a metric, helpful in determining, how similar the data objects are irrespective of their size. We can measure the similarity between two sentences in Python using Cosine Similarity. In cosine similarity, data objects in a dataset are treated as a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f50e8",
   "metadata": {},
   "source": [
    "#### 9. State difference between Euclidean and Manhattan distances ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254cc58f",
   "metadata": {},
   "source": [
    "**Ans:** Euclidean & Hamming distances are used to measure similarity or dissimilarity between two sequences. Euclidean distance is extensively applied in analysis of convolutional codes and Trellis codes.\n",
    "\n",
    "Hamming distance is frequently encountered in the analysis of block codes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9cc89b",
   "metadata": {},
   "source": [
    "The main difference between Euclidean distance and Manhattan distance is the way they measure distance between two points in a multi-dimensional space.\n",
    "\n",
    "Euclidean distance is the straight-line distance between two points in Euclidean space. It is calculated as the square root of the sum of the squared differences between each dimension of the two points. Mathematically, it can be expressed as:\n",
    "\n",
    "**d(x, y) = √(Σ(xi - yi)²)**\n",
    "\n",
    "where x and y are the two points in n-dimensional space, xi and yi are the ith dimensions of x and y, and n is the total number of dimensions.\n",
    "\n",
    "Manhattan distance, also known as city block distance, is the sum of the absolute differences between each dimension of the two points. It is calculated as the sum of the absolute differences between the x and y coordinates along each dimension. Mathematically, it can be expressed as:\n",
    "\n",
    "**d(x, y) = Σ|xi - yi|**\n",
    "\n",
    "where x and y are the two points in n-dimensional space, xi and yi are the ith dimensions of x and y, and n is the total number of dimensions.\n",
    "\n",
    "The key difference between the two distance metrics is that Euclidean distance calculates the shortest possible distance between two points, while Manhattan distance calculates the distance by moving only along the axes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b408ffd",
   "metadata": {},
   "source": [
    "#### 10. Distinguish between feature transformation and feature selection.\n",
    "\n",
    "#### Ans:\n",
    "Feature selection is for filtering irrelevant or redundant features from your dataset. The key difference between feature selection and extraction is that feature selection keeps a subset of the original features while feature extraction creates brand new ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffd741",
   "metadata": {},
   "source": [
    "**Feature transformation** involves converting or modifying the existing features into a new set of features that better represent the underlying data. This can be done by applying mathematical functions, such as logarithmic or exponential functions, to the existing features or by creating new features that are combinations of existing features. The goal of feature transformation is to create a set of features that better captures the underlying patterns in the data and improves the performance of the model. Examples of feature transformation techniques include Principal Component Analysis (PCA), Z-score normalization, and polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8da4dbd",
   "metadata": {},
   "source": [
    "**Feature selection**, on the other hand, involves selecting a subset of the existing features to be used in the model and discarding the rest. The goal of feature selection is to reduce the complexity of the model, improve its interpretability, and prevent overfitting by eliminating irrelevant or redundant features. Feature selection can be done using various techniques, such as filter methods, wrapper methods, and embedded methods. Examples of feature selection techniques include correlation-based feature selection, recursive feature elimination, and L1 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e4aed",
   "metadata": {},
   "source": [
    "#### 11. Make brief notes on any two of the following:\n",
    "\n",
    "  1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "  2. Collection of features using a hybrid approach\n",
    "\n",
    "  3. The width of the silhouette\n",
    "\n",
    "  4. Receiver operating characteristic curve\n",
    "\n",
    "#### 1.SVD (Standard Variable Diameter Diameter)\n",
    "\n",
    "SVD, or Singular Value Decomposition, is one of several techniques that can be used to reduce the dimensionality, i.e., the number of columns, of a data set. Why would we want to reduce the number of dimensions? In predictive analytics, more columns normally means more time required to build models and score data\n",
    "\n",
    "#### 2. Collection of features using a hybrid approach\n",
    "\n",
    "A hybrid feature selection method is proposed for classification in small sample size data sets. The filter step is based on instance learning taking advantage of the small sample size of data. A few candidate feature subsets are generated since their number corresponds to the number of instances.\n",
    "\n",
    "#### 3. The width of the silhouette\n",
    "\n",
    "The silhouette value is a measure of how similar an object is to its own cluster (cohesion) compared to other clusters (separation). The value of the silhouette ranges between [1, -1], where a high value indicates that the object is well matched to its own cluster and poorly matched to neighboring clusters.\n",
    "\n",
    "#### 4. Receiver operating characteristic curve\n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. This curve plots two parameters: True Positive Rate. False Positive Rate.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c339d341",
   "metadata": {},
   "source": [
    "<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/1/13/Roc_curve.svg/220px-Roc_curve.svg.png\" width=\"500\" heigth=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3e670",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090462fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a8d04e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
