{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d19dcb14",
   "metadata": {},
   "source": [
    "# Machine Learning 15 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5920be09",
   "metadata": {},
   "source": [
    "#### 1. Recognize the differences between supervised, semi-supervised, and unsupervised learning ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86065e1",
   "metadata": {},
   "source": [
    "### Ans:\n",
    "\n",
    "**Supervised Learning:** In supervised learning, the machine learning algorithm is trained on a labeled dataset, where each input data point is associated with a known output label or target value. The goal of the algorithm is to learn a mapping between the input features and the output labels, so that it can predict the output label for new, unseen input data points. Examples of supervised learning include image classification, sentiment analysis, and fraud detection.\n",
    "\n",
    "**Unsupervised Learning:** In unsupervised learning, the machine learning algorithm is trained on an unlabeled dataset, where there are no predefined output labels or target values. The goal of the algorithm is to discover hidden patterns, relationships, or structures in the input data, such as clusters, subgroups, or associations. Examples of unsupervised learning include clustering, anomaly detection, and dimensionality reduction.\n",
    "\n",
    "**Semi-Supervised Learning:** Semi-supervised learning is a hybrid of supervised and unsupervised learning, where the machine learning algorithm is trained on a dataset that contains both labeled and unlabeled data points. The goal of the algorithm is to use the labeled data to learn a mapping between the input features and output labels, and then use the unlabeled data to improve the accuracy or generalization ability of the model. Examples of semi-supervised learning include speech recognition, natural language processing, and image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b7a1e9",
   "metadata": {},
   "source": [
    "###### Supervised learning aims to learn a function that, given a sample of data and desired outputs, approximates a function that maps inputs to outputs.\n",
    "\n",
    "##### Semi-supervised learning aims to label unlabeled data points using knowledge learned from a small number of labeled data points.\n",
    "\n",
    "##### Unsupervised learning does not have (or need) any labeled outputs, so its goal is to infer the natural structure present within a set of data points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c9e481",
   "metadata": {},
   "source": [
    "#### 2. Describe in detail any five examples of classification problems ?\n",
    "\n",
    "**Ans:** 5 Examples of Classification Problems :\n",
    "\n",
    "**1.Logistic regression:** Logistic regression is a supervised learning classification algorithm used to predict the probability \n",
    "\n",
    "of a target variable. ... It is one of the simplest ML algorithms that can be used for various classification problems such as \n",
    "\n",
    "spam detection, Diabetes prediction, cancer detection etc.\n",
    "\n",
    "**2.Decision trees:** Decision Trees are a type of Supervised Machine Learning (that is you explain what the input is and what the corresponding output is in the training data) where the data is continuously split according to a certain parameter. The tree can be explained by two entities, namely decision nodes and leaves.\n",
    "\n",
    "**3.Random forest:** Random forest is a Supervised Machine Learning Algorithm that is used widely in Classification and Regression problems. It builds decision trees on different samples and takes their majority vote for classification and average in case of regression. It performs better results for classification problems.\n",
    "\n",
    "**4.XGBoost:** XGBoost is an algorithm that has recently been dominating applied machine learning and Kaggle competitions for structured or tabular data. XGBoost is an implementation of gradient boosted decision trees designed for speed and performance.Why XGBoost must be a part of your machine learning toolkit.\n",
    "\n",
    "**5.Light GBM:** Light Gradient Boosted Machine, or LightGBM for short, is an open-source library that provides an efficient and effective implementation of the gradient boosting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2405aac",
   "metadata": {},
   "source": [
    "#### 3. Describe each phase of the classification process in detail ?\n",
    "\n",
    "\n",
    "**Ans:** Process of classification consists of two phases :\n",
    "\n",
    "1.Construction of the classifier\n",
    "\n",
    "2.Usage of the classifier.\n",
    "\n",
    "A classifier in machine learning is an algorithm that automatically orders or categorizes data into one or more of a set of “classes.” One of the most common examples is an email classifier that scans emails to filter them by class label: Spam or Not Spam.\n",
    "\n",
    "The main goal of the Classification algorithm is to identify the category of a given dataset, and these algorithms are mainly used to predict the output for the categorical data. Multi-class Classifier: If a classification problem has more than two outcomes, then it is called as Multi-class Classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7dfab9",
   "metadata": {},
   "source": [
    "#### 4.4. Go through the SVM model in depth using various scenarios ?\n",
    "\n",
    "**Ans: Support Vector Machine (SVM)|** is a supervised machine learning algorithm that can be used for both classification or \n",
    "\n",
    "regression challenges.\n",
    "\n",
    "Support Vectors are simply the coordinates of individual observation. The SVM classifier is a frontier that best segregates the \n",
    "\n",
    "two classes (hyper-plane/ line).\n",
    "\n",
    "SVM or Support Vector Machine is a linear model for classification and regression problems. It can solve linear and non-linear \n",
    "\n",
    "problems and work well for many practical problems. The idea of SVM is simple: The algorithm creates a line or a hyperplane \n",
    "\n",
    "which separates the data into classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ad6876",
   "metadata": {},
   "source": [
    "#### 5. What are some of the benefits and drawbacks of SVM ?\n",
    "\n",
    "**Ans:** The following are some of the benefits and drawbacks of SVM:\n",
    "\n",
    "#### Benefits:\n",
    "\n",
    "1.SVM works relatively well when there is a clear margin of separation between classes.\n",
    "\n",
    "2.SVM is more effective in high dimensional spaces.\n",
    "\n",
    "3.SVM is effective in cases where the number of dimensions is greater than the number of samples.\n",
    "\n",
    "\n",
    "4.SVM is relatively memory efficient.\n",
    "\n",
    "#### Drawbacks:\n",
    "\n",
    "1.SVM algorithm is not suitable for large data sets.\n",
    "\n",
    "2.SVM does not perform very well when the data set has more noise i.e. target classes are overlapping.\n",
    "\n",
    "3.In cases where the number of features for each data point exceeds the number of training data samples,the SVM will \n",
    "\n",
    "underperform."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd25306",
   "metadata": {},
   "source": [
    "#### 6. Go over the kNN model in depth ?\n",
    "\n",
    "**Ans:** kNN is the simplest machine learning algorithm to understand and also to explain. It is a versatile algorithm i.e. \n",
    "\n",
    "useful for both classification and regression. It has one big advantage is that kNN ha no pre assumption about the data. Let \n",
    "\n",
    "the data speak for itself.\n",
    "\n",
    "The abbreviation KNN stands for **K-Nearest Neighbour**. It is a supervised machine learning algorithm. The algorithm can be used to solve both classification and regression problem statements. The number of nearest neighbours to a new unknown variable that has to be predicted or classified is denoted by the symbol 'K'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e90a06",
   "metadata": {},
   "source": [
    "#### 7. Discuss the kNN algorithm's error rate and validation error ?\n",
    "\n",
    "**Ans:** Training error here is the error you'll have when you input your training set to your KNN as test set. Since your test \n",
    "\n",
    "sample is in the training dataset, it'll choose itself as the closest and never make mistake. For this reason, the training \n",
    "\n",
    "error will be zero when K = 1, irrespective of the dataset.\n",
    "\n",
    "kNN produces predictions by looking at the k nearest neighbours of a case x to predict its y, so that's fine. In particular, \n",
    "\n",
    "the kNN model basically consists of its training cases - but that's the cross validation procedure doesn't care about at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d70fb5b",
   "metadata": {},
   "source": [
    "#### 8. For kNN, talk about how to measure the difference between the test and training results ?\n",
    "\n",
    "\n",
    "**Ans:** KNN can be used for classification — the output is a class membership (predicts a class — a discrete value). An object \n",
    "\n",
    "is classified by a majority vote of its neighbors, with the object being assigned to the class most common among its k nearest \n",
    "\n",
    "neighbors.\n",
    "\n",
    "KNN classifier does not have any specialized training phase as it uses all the training samples for classification and simply \n",
    "\n",
    "stores the results in memory. KNN is a non-parametric algorithm because it does not assume anything about the training data. \n",
    "\n",
    "\n",
    "This makes it useful for problems having non-linear data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4ac3da",
   "metadata": {},
   "source": [
    "#### 9. Create the kNN algorithm ?\n",
    "\n",
    "**Ans:** The k-nearest neighbor algorithm is imported from the scikit-learn package.\n",
    "\n",
    "1.Create feature and target variables.\n",
    "\n",
    "2.Split data into training and test data.\n",
    "\n",
    "3.Generate a k-NN model using neighbors value.\n",
    "\n",
    "4.Train or fit the data into the model.\n",
    "\n",
    "5.Predict the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a988814b",
   "metadata": {},
   "source": [
    "#### 10. What is a decision tree, exactly ? What are the various kinds of nodes? Explain all in depth ?\n",
    "\n",
    "**Ans:** A decision tree is a tree-like model that acts as a decision support tool, visually displaying decisions and their \n",
    "\n",
    "potential outcomes, consequences, and costs. Drawing a decision tree diagram starts from left to right and consists of burst \n",
    "\n",
    "nodes that split into different paths.\n",
    "\n",
    "There are three different types of nodes: chance nodes, decision nodes, and end nodes. A chance node, represented by a circle, \n",
    "\n",
    "shows the probabilities of certain results. A decision node, represented by a square, shows a decision to be made, and an end \n",
    "\n",
    "node shows the final outcome of a decision path."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa03272",
   "metadata": {},
   "source": [
    "#### 11. Describe the different ways to scan a decision tree ?\n",
    "\n",
    "**Ans:** In a decision tree analysis, the decision-maker has usually to proceed through the following six steps:\n",
    "\n",
    "1.Define the problem in structured terms.\n",
    "\n",
    "2.Model the decision process.\n",
    "\n",
    "3.Apply the appropriate probability values and financial data.\n",
    "\n",
    "4.Solve the decision tree.\n",
    "\n",
    "5.Perform sensitivity analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa8a4d8",
   "metadata": {},
   "source": [
    "#### 12. Describe in depth the decision tree algorithm ?\n",
    "\n",
    "**Ans:** Decision Tree algorithm belongs to the family of supervised learning algorithms. The goal of using a Decision Tree is to create a training model that can use to predict the class or value of the target variable by learning simple decision rules inferred from prior data(training data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3569ab0",
   "metadata": {},
   "source": [
    "Tree depth is a measure of how many splits a tree can make before coming to a prediction. This process could be continued \n",
    "\n",
    "further with more splitting until the tree is as pure as possible. The problem with many repetitions of this process is that \n",
    "\n",
    "this can lead to a very deep classification tree with many nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8986241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "000aa854",
   "metadata": {},
   "source": [
    "#### 13. In a decision tree, what is inductive bias? What would you do to stop overfitting ?\n",
    "\n",
    "\n",
    "**Ans:** The inductive bias (also known as learning bias) of a learning algorithm is the set of assumptions that the learner \n",
    "\n",
    "uses to predict outputs of given inputs that it has not encountered. The kind of necessary assumptions about the nature of the \n",
    "\n",
    "target function are subsumed in the phrase inductive bias.\n",
    "\n",
    "Overfitting makes the model relevant to its data set only, and irrelevant to any other data sets. Some of the methods used to \n",
    "\n",
    "prevent overfitting include ensembling, data augmentation, data simplification, and cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099b79f8",
   "metadata": {},
   "source": [
    "#### 14.Explain advantages and disadvantages of using a decision tree ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d858e11d",
   "metadata": {},
   "source": [
    "### Ans:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0cb0c2",
   "metadata": {},
   "source": [
    "#### Advantages:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e067cd4",
   "metadata": {},
   "source": [
    "1.It can be used for both classification and regression problems: Decision trees can be used to predict both continuous and discrete values i.e. they work well in both regression and classification tasks.\n",
    "\n",
    "2.They are very fast and efficient compared to KNN and other classification algorithms.\n",
    "\n",
    "3.Easy to understand, interpret, visualize.\n",
    "\n",
    "4.The data type of decision tree can handle any type of data whether it is numerical or categorical, or boolean.\n",
    "\n",
    "5.Normalization is not required in the Decision Tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94431d9",
   "metadata": {},
   "source": [
    "#### Disadvantages:\n",
    "\n",
    "1.Concerning the decision tree split for numerical variables millions of records: The time complexity right for operating this operation is very huge keep on increasing as the number of records gets increased decision tree with to numerical variables takes a lot of time for training.\n",
    "\n",
    "2.Similarly, this happens in techniques like random forests, XGBoost.\n",
    "\n",
    "3.Decision tree for many features: Take more time for training-time complexity to increase as the input increases.\n",
    "\n",
    "4.Growing with the tree from the training set: Overfit pruning (pre, post), ensemble method random forest.\n",
    "\n",
    "5.Method of overfitting: If we discuss overfitting, it is one of the most difficult methods for decision tree models. The overfitting problem can be solved by setting constraints on the parameters model and pruning method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b54d31",
   "metadata": {},
   "source": [
    "#### 15. Describe in depth the problems that are suitable for decision tree learning ?\n",
    "\n",
    "**Ans:** Appropriate Problems for Decision Tree Learning are: Instances are represented by attribute-value pairs. The target function has discrete output values.\n",
    "\n",
    "1.Disjunctive descriptions may be required.\n",
    "\n",
    "2.The training data may contain errors.\n",
    "\n",
    "3.The training data may contain missing attribute values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca8b4c7",
   "metadata": {},
   "source": [
    "#### 16. Describe in depth the random forest model. What distinguishes a random forest ?\n",
    "\n",
    "**Ans:** The random forest is a classification algorithm consisting of many decisions trees. It uses bagging and feature \n",
    "\n",
    "randomness when building each individual tree to try to create an uncorrelated forest of trees whose prediction by committee is \n",
    "\n",
    "more accurate than that of any individual tree. The fundamental difference is that in Random forests, only a subset of features \n",
    "\n",
    "are selected at random out of the total and the best split feature from the subset is used to split each node in a tree, unlike \n",
    "\n",
    "in bagging where all features are considered for splitting a node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a74def",
   "metadata": {},
   "source": [
    "#### 17. In a random forest, talk about OOB error and variable value ?\n",
    "\n",
    "**Ans:** The out-of-bag (OOB) error is the average error for each calculated using predictions from the trees that do not \n",
    "\n",
    "contain in their respective bootstrap sample. There are two measures of importance given for each variable in the random \n",
    "\n",
    "forest. The first measure is based on how much the accuracy decreases when the variable is excluded.The second measure is based \n",
    "\n",
    "on the decrease of Gini impurity when a variable is chosen to split a node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5656b4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
